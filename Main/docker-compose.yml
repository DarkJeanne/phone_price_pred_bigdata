services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.2
    container_name: zookeeper
    restart: unless-stopped
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SERVER_ID: 1
    healthcheck:
      test: echo ruok | nc -w 2 localhost 2181 || exit 1
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - data_pipeline_net

  kafka:
    image: confluentinc/cp-kafka:7.3.2
    container_name: kafka
    restart: unless-stopped
    ports:
      - "${KAFKA_BOOTSTRAP_SERVERS_EXTERNAL_PORT:-9094}:9094"
      - "9092:9092"
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9094
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    healthcheck:
      test: nc -z kafka 9092 || exit 1
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - data_pipeline_net

  hbase:
    image: harisekhon/hbase:1.4
    container_name: hbase
    hostname: hbase
    restart: unless-stopped
    ports:
      - "9090:9090"
      - "9095:9095"
      - "16010:16010"
    environment:
      HBASE_MANAGES_ZK: "false"
      HBASE_ZOOKEEPER_QUORUM: zookeeper
      HBASE_ZOOKEEPER_PROPERTY_CLIENTPORT: 2181
    entrypoint: ["/bin/bash"]
    volumes:
      - ./start-hbase.sh:/start-hbase.sh
    tty: true
    depends_on:
      zookeeper:
        condition: service_healthy
    command: ["/start-hbase.sh"]
    networks:
      - data_pipeline_net

  hadoop:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop
    hostname: hadoop
    restart: unless-stopped
    ports:
      - "9870:9870"
      - "9000:9000"
      - "9864:9864"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=hadoop
      - CORE_CONF_fs_defaultFS=hdfs://hadoop:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_rpc_bind_host=0.0.0.0
      - HDFS_CONF_dfs_namenode_http_bind_host=0.0.0.0
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
    healthcheck:
      test: curl -f http://localhost:9870 || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - data_pipeline_net

  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-datanode
    restart: unless-stopped
    depends_on:
      - hadoop
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop:9000
      - HDFS_CONF_dfs_datanode_data_dir=/hadoop/dfs/data
      - SERVICE_PRECONDITION=hadoop:9870
    healthcheck:
      test: curl -f http://localhost:9864 || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - data_pipeline_net

  spark-master:
    build:
      context: .
      dockerfile: spark-with-deps.Dockerfile
    container_name: spark-master
    hostname: spark-master
    ports:
      - "${SPARK_MASTER_WEBUI_PORT:-8080}:8080"
      - "7077:7077"
    environment:
      SPARK_PUBLIC_DNS: localhost
    volumes:
      - ./Lambda/Batch_layer:/opt/spark/apps/batch_layer
      - ./Lambda/ML_operations:/opt/spark/apps/ml_ops
      - ./Lambda/transform.py:/opt/spark/apps/transform.py
    env_file: .env
    command: "/opt/spark/bin/spark-class org.apache.spark.deploy.master.Master"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 45s
    networks:
      - data_pipeline_net

  spark-worker:
    build:
      context: .
      dockerfile: spark-with-deps.Dockerfile
    container_name: spark-worker
    depends_on:
      spark-master:
        condition: service_healthy
    ports:
      - "8082:8081"
    environment:
      SPARK_MASTER: ${SPARK_MASTER_URL_INTERNAL:-spark://spark-master:7077}
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
    volumes:
      - ./Lambda/Batch_layer:/opt/spark/apps/batch_layer
      - ./Lambda/ML_operations:/opt/spark/apps/ml_ops
      - ./Lambda/transform.py:/opt/spark/apps/transform.py
    env_file: .env
    command: "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"
    networks:
      - data_pipeline_net

  spark-batch-job:
    build:
      context: .
      dockerfile: spark-with-deps.Dockerfile
    container_name: spark-batch-job
    depends_on:
      spark-master:
        condition: service_healthy
      hadoop:
        condition: service_healthy
    volumes:
      - ./Lambda/Batch_layer:/opt/spark/apps/batch_layer
      - ./Lambda/ML_operations:/opt/spark/apps/ml_ops
      - ./Lambda/transform.py:/opt/spark/apps/transform.py
    environment:
      - MONGO_CONNECTION_URI=mongodb+srv://nongvandenhoc:hoangtu1234@cluster0.tqiqnnk.mongodb.net/phone_price_pred?retryWrites=true&w=majority
      - MONGO_DB_NAME=phone_price_pred
      - MONGO_COLLECTION_NAME=smartphones
      - HDFS_NAMENODE_HOST_INTERNAL=hadoop
    command: >
      /bin/bash -c "
        rm -rf /home/spark/.ivy2/cache/* && \
        cd /opt/spark/apps && \
        cp ml_ops/xgb_model.pkl batch_layer/ && \
        /opt/spark/bin/spark-submit --verbose --master spark://spark-master:7077 --deploy-mode client --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --py-files transform.py batch_layer/spark_tranformation.py
      "
    networks:
      - data_pipeline_net

  stream-processor:
    build:
      context: ./Lambda
      dockerfile: stream_processor.Dockerfile
    container_name: stream-processor
    depends_on:
      kafka:
        condition: service_healthy
      hbase:
        condition: service_started
    env_file: .env
    environment:
      KAFKA_BROKER_INTERNAL: kafka:9092
      KAFKA_TOPIC_NAME: phone_data
      HBASE_HOST: hbase
      HBASE_PORT: 9090
    volumes:
      - ./Lambda/Stream_layer:/opt/app/Stream_layer
      - ./Lambda/Stream_data:/opt/app/Stream_data
      - ./Lambda/ML_operations:/opt/app/ML_operations
      - ./Lambda/transform.py:/opt/app/transform.py
      - ./Lambda/producer.py:/opt/app/producer.py
    networks:
      - data_pipeline_net

  batch-processor:
    build:
      context: ./Lambda
      dockerfile: batch_processor.Dockerfile
    container_name: batch-processor
    depends_on:
      kafka:
        condition: service_healthy
      hadoop:
        condition: service_healthy
    env_file: .env
    environment:
      KAFKA_BROKER_INTERNAL: kafka:9092
      KAFKA_SMARTPHONE_TOPIC: smartphoneTopic
      HDFS_NAMENODE_WEBUI_HOST_INTERNAL: hadoop
      HDFS_NAMENODE_WEBUI_PORT: 9870
    volumes:
      - ./Lambda/Batch_layer:/app/batch_layer
      - ./Lambda/Stream_data:/app/Stream_data
      - ./Lambda/transform.py:/app/transform.py
      - ./Lambda/producer.py:/app/producer.py
    networks:
      - data_pipeline_net

  flask-app:
    build:
      context: ./Lambda/real_time_web_app(Flask)
      dockerfile: Dockerfile
    container_name: flask-app
    ports:
      - "5001:5001"
    depends_on:
      hbase:
        condition: service_started
    environment:
      FLASK_APP_PORT: "5001"
      HBASE_HOST: "hbase"
      HBASE_PORT: "9090"
      HBASE_SMARTPHONE_TABLE: "smartphone"
    networks:
      - data_pipeline_net

volumes:
  hadoop_namenode:
  hadoop_datanode:

networks:
  data_pipeline_net:
    driver: bridge
